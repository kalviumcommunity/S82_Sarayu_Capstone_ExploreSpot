{"version":3,"file":"base.d.ts","names":["____________langchain_core_dist_utils_stream_js0","ClientOptions","OpenAI","OpenAIClient","AIMessageChunk","BaseMessage","ChatGeneration","BaseChatModel","LangSmithParams","BaseChatModelParams","BaseChatModelCallOptions","BaseFunctionCallOptions","BaseLanguageModelInput","StructuredOutputMethodOptions","ModelProfile","Runnable","InteropZodType","OpenAICallOptions","OpenAIChatInput","OpenAICoreRequestOptions","ChatOpenAIResponseFormat","ResponseFormatConfiguration","OpenAIVerbosityParam","OpenAIApiKey","OpenAIToolChoice","ChatOpenAIToolType","ResponsesToolChoice","OpenAILLMOutput","BaseChatOpenAICallOptions","Chat","ChatCompletionStreamOptions","ChatCompletionModality","Array","ChatCompletionAudioParam","ChatCompletionPredictionContent","Reasoning","ChatCompletionCreateParams","BaseChatOpenAIFields","Partial","BaseChatOpenAI","CallOptions","Record","Omit","ChatCompletionTool","_langchain_core_messages0","MessageStructure","IterableReadableStream","Promise","Function","ChatCompletionFunctionCallOption","RunOutput"],"sources":["../../src/chat_models/base.d.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatGeneration } from \"@langchain/core/outputs\";\nimport { BaseChatModel, type LangSmithParams, type BaseChatModelParams, BaseChatModelCallOptions } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseFunctionCallOptions, type BaseLanguageModelInput, type StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { ModelProfile } from \"@langchain/core/language_models/profile\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { type OpenAICallOptions, type OpenAIChatInput, type OpenAICoreRequestOptions, type ChatOpenAIResponseFormat, ResponseFormatConfiguration, OpenAIVerbosityParam, type OpenAIApiKey } from \"../types.js\";\nimport { OpenAIToolChoice, ChatOpenAIToolType, ResponsesToolChoice } from \"../utils/tools.js\";\ninterface OpenAILLMOutput {\n    tokenUsage: {\n        completionTokens?: number;\n        promptTokens?: number;\n        totalTokens?: number;\n    };\n}\nexport type { OpenAICallOptions, OpenAIChatInput };\nexport interface BaseChatOpenAICallOptions extends BaseChatModelCallOptions, BaseFunctionCallOptions {\n    /**\n     * Additional options to pass to the underlying axios request.\n     */\n    options?: OpenAICoreRequestOptions;\n    /**\n     * A list of tools that the model may use to generate responses.\n     * Each tool can be a function, a built-in tool, or a custom tool definition.\n     * If not provided, the model will not use any tools.\n     */\n    tools?: ChatOpenAIToolType[];\n    /**\n     * Specifies which tool the model should use to respond.\n     * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n     * If not set, the model will decide which tool to use automatically.\n     */\n    // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n    tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n    /**\n     * Adds a prompt index to prompts passed to the model to track\n     * what prompt is being used for a given generation.\n     */\n    promptIndex?: number;\n    /**\n     * An object specifying the format that the model must output.\n     */\n    response_format?: ChatOpenAIResponseFormat;\n    /**\n     * When provided, the completions API will make a best effort to sample\n     * deterministically, such that repeated requests with the same `seed`\n     * and parameters should return the same result.\n     */\n    seed?: number;\n    /**\n     * Additional options to pass to streamed completions.\n     * If provided, this takes precedence over \"streamUsage\" set at\n     * initialization time.\n     */\n    stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n    /**\n     * The model may choose to call multiple functions in a single turn. You can\n     * set parallel_tool_calls to false which ensures only one tool is called at most.\n     * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n     */\n    parallel_tool_calls?: boolean;\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the tool definition. If `true`, the input schema will also be\n     * validated according to\n     * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n     *\n     * If `false`, input schema will not be validated and model output will not\n     * be validated.\n     *\n     * If `undefined`, `strict` argument will not be passed to the model.\n     */\n    strict?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Static predicted output content, such as the content of a text file that is being regenerated.\n     * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n     */\n    prediction?: OpenAIClient.ChatCompletionPredictionContent;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. If these options are set, the responses API will be used to fulfill the request.\n     *\n     * These options will be ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport interface BaseChatOpenAIFields extends Partial<OpenAIChatInput>, BaseChatModelParams {\n    /**\n     * Optional configuration options for the OpenAI client.\n     */\n    configuration?: ClientOptions;\n}\n/** @internal */\nexport declare abstract class BaseChatOpenAI<CallOptions extends BaseChatOpenAICallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements Partial<OpenAIChatInput> {\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n    n?: number;\n    logitBias?: Record<string, number>;\n    model: string;\n    modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n    stop?: string[];\n    stopSequences?: string[];\n    user?: string;\n    timeout?: number;\n    streaming: boolean;\n    streamUsage: boolean;\n    maxTokens?: number;\n    logprobs?: boolean;\n    topLogprobs?: number;\n    apiKey?: OpenAIApiKey;\n    organization?: string;\n    __includeRawResponse?: boolean;\n    /** @internal */\n    client: OpenAIClient;\n    /** @internal */\n    clientConfig: ClientOptions;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n     * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n     * OpenAI organization or project. This must be configured directly with OpenAI.\n     *\n     * See:\n     * https://platform.openai.com/docs/guides/your-data\n     * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean | undefined;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n    protected defaultOptions: CallOptions;\n    _llmType(): string;\n    static lc_name(): string;\n    get callKeys(): string[];\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): Record<string, string>;\n    get lc_serializable_keys(): string[];\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    /** @ignore */\n    _identifyingParams(): Omit<OpenAIClient.Chat.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<OpenAIClient.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    constructor(fields?: BaseChatOpenAIFields);\n    /**\n     * Returns backwards compatible reasoning parameters from constructor params and call options\n     * @internal\n     */\n    protected _getReasoningParams(options?: this[\"ParsedCallOptions\"]): OpenAIClient.Reasoning | undefined;\n    /**\n     * Returns an openai compatible response format from a set of options\n     * @internal\n     */\n    protected _getResponseFormat(resFormat?: CallOptions[\"response_format\"]): ResponseFormatConfiguration | undefined;\n    protected _combineCallOptions(additionalOptions?: this[\"ParsedCallOptions\"]): this[\"ParsedCallOptions\"];\n    /** @internal */\n    _getClientOptions(options: OpenAICoreRequestOptions | undefined): OpenAICoreRequestOptions;\n    // TODO: move to completions class\n    protected _convertChatOpenAIToolToCompletionsTool(tool: ChatOpenAIToolType, fields?: {\n        strict?: boolean;\n    }): OpenAIClient.ChatCompletionTool;\n    bindTools(tools: ChatOpenAIToolType[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;\n    stream(input: BaseLanguageModelInput, options?: CallOptions): Promise<import(\"../../../../langchain-core/dist/utils/stream.js\").IterableReadableStream<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>>;\n    invoke(input: BaseLanguageModelInput, options?: CallOptions): Promise<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>;\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput;\n    getNumTokensFromMessages(messages: BaseMessage[]): Promise<{\n        totalCount: number;\n        countPerMessage: number[];\n    }>;\n    /** @internal */\n    protected _getNumTokensFromGenerations(generations: ChatGeneration[]): Promise<number>;\n    /** @internal */\n    protected _getEstimatedTokenCountFromPrompt(messages: BaseMessage[], functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[], function_call?: \"none\" | \"auto\" | OpenAIClient.Chat.ChatCompletionFunctionCallOption): Promise<number>;\n    /**\n     * Return profiling information for the model.\n     *\n     * Provides information about the model's capabilities and constraints,\n     * including token limits, multimodal support, and advanced features like\n     * tool calling and structured output.\n     *\n     * @returns {ModelProfile} An object describing the model's capabilities and constraints\n     *\n     * @example\n     * ```typescript\n     * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n     * const profile = model.profile;\n     * console.log(profile.maxInputTokens); // 128000\n     * console.log(profile.imageInputs); // true\n     * ```\n     */\n    get profile(): ModelProfile;\n    /** @internal */\n    protected _getStructuredOutputMethod(config: StructuredOutputMethodOptions<boolean>): string | undefined;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<boolean>): Runnable<BaseLanguageModelInput, RunOutput> | Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\n"],"mappings":";;;;;;;;;;;;;;UAUU2B,eAAAA;;;;IAAAA,WAAAA,CAAAA,EAAe,MAAA;EAQRC,CAAAA;;AAULH,UAVKG,yBAAAA,SAAkClB,wBAUvCe,EAViEd,uBAUjEc,CAAAA;EAAkB;;;EAgBgB,OAYzBtB,CAAAA,EAlCPgB,wBAkCyBW;EAA2B;;;;;EAmD5B,KAKnB3B,CAAAA,EApFPsB,kBAoFyBW,EAAAA;EAA0B;;;AA9FqC;AA0GpG;EAAqC;EAAA,WAAiBlB,CAAAA,EAzFpCM,gBAyFoCN,GAzFjBQ,mBAyFiBR;EAAe;;;AAAsB;EAO7DqB,WAAAA,CAAAA,EAAAA,MAAc;EAAA;;;EAAiF,eAAEnC,CAAAA,EAvFzGgB,wBAuFyGhB;EAAc;;;;;EAsBrH,IAENH,CAAAA,EAAAA,MAAAA;EAAa;;;;;EAyBgC,cAU/CqB,CAAAA,EAtIKnB,QAAAA,CAAa0B,IAAAA,CAAKC,2BAsIvBR;EAAoB;;;;;EAaN,mBAEtBrB,CAAAA,EAAAA,OAAAA;EAAa;;;;;;;;;;;EAwBkB,MAClBwB,CAAAA,EAAAA,OAAAA;EAAkB;;;;;;;;;;;;EACkC,UACvDb,CAAAA,EAjJDoB,KAiJCpB,CAjJKT,QAAAA,CAAa0B,IAAAA,CAAKE,sBAiJvBnB,CAAAA;EAAsB;;;;;EAEY,KAAKe,CAAAA,EA7I7CxB,QAAAA,CAAa0B,IAAAA,CAAKI,wBA6I2BN;EAAe;;;;EAMU,UAExBtB,CAAAA,EAhJzCF,QAAAA,CAAa+B,+BAgJ4B7B;EAAW;;;;;;;;EAuBkC,SAAxBW,CAAAA,EA9J/Db,QAAAA,CAAagC,SA8JkDnB;EAAc;;;;EAEyB,YAA1CD,CAAAA,EA3JzDZ,QAAAA,CAAa0B,IAAAA,CAAKO,0BA2JuCrB,CAAAA,cAAAA,CAAAA;EAAQ;;;;;EAKvE,cAAwBF,CAAAA,EAAAA,MAAAA;EAA6B;;;EAEzC,SAFkDE,CAAAA,EAtJ3DO,oBAsJ2DP;;AAM/B0B,UA1J3BJ,oBAAAA,SAA6BC,OA0JFG,CA1JUvB,eA0JVuB,CAAAA,EA1J4BhC,mBA0J5BgC,CAAAA;EAAM;;;EAErC,aAAwB5B,CAAAA,EAxJjBZ,aAwJiBY;;;AAAyCE,uBArJhDwB,cAqJgDxB,CAAAA,oBArJba,yBAqJab,CAAAA,SArJsBR,aAqJtBQ,CArJoCyB,WAqJpCzB,EArJiDX,cAqJjDW,CAAAA,YArJ4EuB,OAqJ5EvB,CArJoFG,eAqJpFH,CAAAA,CAAAA;EAAQ,WAA+CH,CAAAA,EAAAA,MAAAA;EAAsB,IAC9IP,CAAAA,EAAAA,MAAAA;EAAW,gBACR6C,CAAAA,EAAAA,MAAAA;EAAS,eAFmGnC,CAAAA,EAAAA,MAAAA;EAAQ,CAAA,CAAA,EArJhCR,MAAAA;EAAa,SAAyC+B,CAAAA,EAM1IG,MAN0IH,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EAAO,KAAA,EAAA,MAAA;gBAQ/IpB;;;;;;;;;;WAULK;;;;UAIDpB;;gBAEMF;;;;;;UAMNE,QAAAA,CAAa0B,IAAAA,CAAKI;eACbD,MAAM7B,QAAAA,CAAa0B,IAAAA,CAAKE;cACzB5B,QAAAA,CAAagC;;;;;;;;;;;;;;;;;iBAiBVhC,QAAAA,CAAa0B,IAAAA,CAAKO;;;;;;;;;;cAUrBd;4BACckB;;;;;;;;oBAQRC;;mDAE+BjC;;wBAE3BkC,KAAKvC,QAAAA,CAAa0B,IAAAA,CAAKO;;MAEzCnC;;;;uBAIiByC,KAAKvC,QAAAA,CAAaiC;;MAEnCnC;uBACiBoC;;;;;sEAK+ClC,QAAAA,CAAagC;;;;;2CAKxCK,iCAAiCnB;;;6BAG/CF,uCAAuCA;;0DAEVM;;MAEpDtB,QAAAA,CAAawC;mBACAlB,+BAA+Ba,QAAQE,eAAezB,SAASH,wBAAwBR,gBAAgBoC;gBAC1G5B,kCAAkC4B,cAAcO,QAAuG,uBAAd3C,eAA5FwC,yBAAAA,CAA8IC,gBAAAA;gBAC3LjC,kCAAkC4B,cAAcO,QAAQ3C,eAAXwC,yBAAAA,CAA6DC,gBAAAA;;mCAEvFlB,oBAAoBA;qCAClBtB,gBAAgB0C;;;;;sDAKCzC,mBAAmByC;;wDAEjB1C,2BAA2BF,QAAAA,CAAa0B,IAAAA,CAAKO,0BAAAA,CAA2BY,8CAA8C7C,QAAAA,CAAa0B,IAAAA,CAAKoB,mCAAmCF;;;;;;;;;;;;;;;;;;iBAkBlNjC;;+CAE8BD;;;oBAG3B4B,sBAAsBA,mCAAmCzB,eAAekC;;IAEvFT,8BAA8B5B,uCAAuCE,SAASH,wBAAwBsC;;;oBAGvFT,sBAAsBA,mCAAmCzB,eAAekC;;IAEvFT,8BAA8B5B,sCAAsCE,SAASH;SACvEP;YACG6C;;;;oBAIMT,sBAAsBA,mCAAmCzB,eAAekC;;IAEvFT,8BAA8B5B,yCAAyCE,SAASH,wBAAwBsC,aAAanC,SAASH;SACxHP;YACG6C"}